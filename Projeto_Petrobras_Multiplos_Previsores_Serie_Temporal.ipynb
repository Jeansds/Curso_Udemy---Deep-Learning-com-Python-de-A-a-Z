{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projeto_Petrobras_Multiplos_Previsores_Serie_Temporal",
      "provenance": [],
      "authorship_tag": "ABX9TyP4GVLz1vIa+eC7VEGzxrHz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeansds/Curso_Udemy---Deep-Learning-com-Python-de-A-a-Z/blob/master/Projeto_Petrobras_Multiplos_Previsores_Serie_Temporal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZWUaGQLb2Od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp5aDi5Yb-e5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base = pd.read_csv('/content/sample_data/petr4_treinamento.csv')\n",
        "base = base.dropna()\n",
        "base_treinamento = base.iloc[:, 1:7].values\n",
        "normalizador = MinMaxScaler(feature_range = (0, 1))\n",
        "base_treinamento_normalizada = normalizador.fit_transform(base_treinamento)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ07OjuSc7kH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "previsores = []\n",
        "preco_real = []\n",
        "for i in range(90, len(base_treinamento_normalizada)):\n",
        "  previsores.append(base_treinamento_normalizada[i - 90: i, 0: 6])\n",
        "  preco_real.append(base_treinamento_normalizada[i, 0])\n",
        "previsores, preco = np.array(previsores), np.array(preco_real)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV1GcUBPebz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "c29ca619-23db-4569-d063-01a9953e50c9"
      },
      "source": [
        "regressor = Sequential()\n",
        "regressor.add(LSTM(units = 100, return_sequences = True, input_shape = (previsores.shape[1], previsores.shape[2])))#return_sequences passa a informacao para as proxima camada LSTM\n",
        "regressor.add(Dropout(0.3))\n",
        "\n",
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.3))\n",
        "\n",
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.3))\n",
        "\n",
        "regressor.add(LSTM(units = 50))\n",
        "regressor.add(Dropout(0.3))\n",
        "\n",
        "regressor.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error',\n",
        "                  metrics = ['mean_absolute_error'])#testando com adam porem é possivel usar o rmsprop"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jg8vL_afENm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor = 'loss', min_delta = 1e-10, \n",
        "                   patience = 10, verbose = 1) #ele percebe quando o resultado para de melhorar e para o treinamento 10/10 Função\n",
        "rlr = ReduceLROnPlateau(monitor = 'loss', factor = 0.2,\n",
        "                        patience = 5, verbose = 1)#reduz a taxa de aprendizagem para melhorar o valor da metrica se não perceber melhora\n",
        "mcp = ModelCheckpoint(filepath = 'pesos.h5', monitor = 'loss',\n",
        "                      save_best_only = True, verbose = 1)#salva os pesos do modelo a cada epoch assim voce pode pegar o melhor modelo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEStB_FJkyIM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb6744bc-aee7-4101-9c8f-7d9d3155638f"
      },
      "source": [
        "regressor.fit(previsores, preco_real, epochs = 100, batch_size = 32,\n",
        "              callbacks = [es, rlr, mcp])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1152/1152 [==============================] - 14s 12ms/step - loss: 0.0215 - mean_absolute_error: 0.1115\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.02155, saving model to pesos.h5\n",
            "Epoch 2/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0053 - mean_absolute_error: 0.0566\n",
            "\n",
            "Epoch 00002: loss improved from 0.02155 to 0.00531, saving model to pesos.h5\n",
            "Epoch 3/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0042 - mean_absolute_error: 0.0504\n",
            "\n",
            "Epoch 00003: loss improved from 0.00531 to 0.00417, saving model to pesos.h5\n",
            "Epoch 4/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0039 - mean_absolute_error: 0.0489\n",
            "\n",
            "Epoch 00004: loss improved from 0.00417 to 0.00390, saving model to pesos.h5\n",
            "Epoch 5/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0032 - mean_absolute_error: 0.0438\n",
            "\n",
            "Epoch 00005: loss improved from 0.00390 to 0.00321, saving model to pesos.h5\n",
            "Epoch 6/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0032 - mean_absolute_error: 0.0429\n",
            "\n",
            "Epoch 00006: loss improved from 0.00321 to 0.00316, saving model to pesos.h5\n",
            "Epoch 7/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0031 - mean_absolute_error: 0.0423\n",
            "\n",
            "Epoch 00007: loss improved from 0.00316 to 0.00311, saving model to pesos.h5\n",
            "Epoch 8/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0028 - mean_absolute_error: 0.0410\n",
            "\n",
            "Epoch 00008: loss improved from 0.00311 to 0.00283, saving model to pesos.h5\n",
            "Epoch 9/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0028 - mean_absolute_error: 0.0406\n",
            "\n",
            "Epoch 00009: loss improved from 0.00283 to 0.00281, saving model to pesos.h5\n",
            "Epoch 10/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0027 - mean_absolute_error: 0.0403\n",
            "\n",
            "Epoch 00010: loss improved from 0.00281 to 0.00274, saving model to pesos.h5\n",
            "Epoch 11/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0028 - mean_absolute_error: 0.0405\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.00274\n",
            "Epoch 12/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0025 - mean_absolute_error: 0.0390\n",
            "\n",
            "Epoch 00012: loss improved from 0.00274 to 0.00252, saving model to pesos.h5\n",
            "Epoch 13/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0023 - mean_absolute_error: 0.0363\n",
            "\n",
            "Epoch 00013: loss improved from 0.00252 to 0.00233, saving model to pesos.h5\n",
            "Epoch 14/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0022 - mean_absolute_error: 0.0365\n",
            "\n",
            "Epoch 00014: loss improved from 0.00233 to 0.00224, saving model to pesos.h5\n",
            "Epoch 15/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0021 - mean_absolute_error: 0.0345\n",
            "\n",
            "Epoch 00015: loss improved from 0.00224 to 0.00206, saving model to pesos.h5\n",
            "Epoch 16/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0020 - mean_absolute_error: 0.0348\n",
            "\n",
            "Epoch 00016: loss improved from 0.00206 to 0.00199, saving model to pesos.h5\n",
            "Epoch 17/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0018 - mean_absolute_error: 0.0318\n",
            "\n",
            "Epoch 00017: loss improved from 0.00199 to 0.00178, saving model to pesos.h5\n",
            "Epoch 18/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0020 - mean_absolute_error: 0.0340\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.00178\n",
            "Epoch 19/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0016 - mean_absolute_error: 0.0304\n",
            "\n",
            "Epoch 00019: loss improved from 0.00178 to 0.00158, saving model to pesos.h5\n",
            "Epoch 20/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0016 - mean_absolute_error: 0.0300\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.00158\n",
            "Epoch 21/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0017 - mean_absolute_error: 0.0306\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.00158\n",
            "Epoch 22/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0017 - mean_absolute_error: 0.0308\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.00158\n",
            "Epoch 23/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0016 - mean_absolute_error: 0.0306\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.00158\n",
            "Epoch 24/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0017 - mean_absolute_error: 0.0314\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.00158\n",
            "Epoch 25/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0014 - mean_absolute_error: 0.0290\n",
            "\n",
            "Epoch 00025: loss improved from 0.00158 to 0.00144, saving model to pesos.h5\n",
            "Epoch 26/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0273\n",
            "\n",
            "Epoch 00026: loss improved from 0.00144 to 0.00133, saving model to pesos.h5\n",
            "Epoch 27/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0269\n",
            "\n",
            "Epoch 00027: loss improved from 0.00133 to 0.00124, saving model to pesos.h5\n",
            "Epoch 28/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0266\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.00124\n",
            "Epoch 29/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0271\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.00124\n",
            "Epoch 30/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0271\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.00124\n",
            "Epoch 31/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0273\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.00124\n",
            "Epoch 32/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0265\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.00124\n",
            "Epoch 33/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0266\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.00124\n",
            "Epoch 34/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0259\n",
            "\n",
            "Epoch 00034: loss improved from 0.00124 to 0.00123, saving model to pesos.h5\n",
            "Epoch 35/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0262\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.00123\n",
            "Epoch 36/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0014 - mean_absolute_error: 0.0274\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.00123\n",
            "Epoch 37/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0266\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.00123\n",
            "Epoch 38/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0270\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.00123\n",
            "Epoch 39/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0263\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.00123\n",
            "Epoch 40/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0264\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.00123\n",
            "Epoch 41/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0263\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.00123\n",
            "Epoch 42/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0263\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.00123\n",
            "Epoch 43/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0258\n",
            "\n",
            "Epoch 00043: loss improved from 0.00123 to 0.00120, saving model to pesos.h5\n",
            "Epoch 44/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0267\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.00120\n",
            "Epoch 45/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0261\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.00120\n",
            "Epoch 46/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0264\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.00120\n",
            "Epoch 47/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0255\n",
            "\n",
            "Epoch 00047: loss improved from 0.00120 to 0.00120, saving model to pesos.h5\n",
            "Epoch 48/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0264\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.00120\n",
            "Epoch 49/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0266\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.00120\n",
            "Epoch 50/100\n",
            "1152/1152 [==============================] - 12s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0269\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.00120\n",
            "Epoch 51/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0264\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.00120\n",
            "Epoch 52/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0273\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.00120\n",
            "Epoch 53/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0012 - mean_absolute_error: 0.0258\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.00120\n",
            "Epoch 54/100\n",
            "1152/1152 [==============================] - 12s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0270\n",
            "\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.00120\n",
            "Epoch 55/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0260\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.00120\n",
            "Epoch 56/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0264\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.00120\n",
            "Epoch 57/100\n",
            "1152/1152 [==============================] - 11s 10ms/step - loss: 0.0013 - mean_absolute_error: 0.0267\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.00120\n",
            "Epoch 00057: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f70cbf63b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOPxN9wWlpN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_teste = pd.read_csv('/content/sample_data/petr4_teste.csv')\n",
        "preco_real_teste = base_teste.iloc[:, 1:2].values\n",
        "\n",
        "frames = [base, base_teste]\n",
        "base_completa = pd.concat(frames)\n",
        "base_completa = base_completa.drop('Date', axis = 1)\n",
        "entradas = base_completa[len(base_completa) - len(base_teste)- 90:].values\n",
        "entradas = normalizador.transform(entradas)# normalização das entradas valores entre 0 e 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYgWKyvwmeOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_teste = []\n",
        "for i in range(90, len(entradas)):\n",
        "  X_teste.append(entradas[i-90: i, 0:6])\n",
        "X_teste = np.array(X_teste)\n",
        "previsores = regressor.predict(X_teste)\n",
        "normalizador_previsor = MinMaxScaler(feature_range = (0, 1))\n",
        "normalizador_previsor.fit_transform(base_treinamento[:,0: 1])\n",
        "previsores = normalizador_previsor.inverse_transform(previsores)# desnormaliza os dados\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB1knbyunefq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "35cf1bc8-592a-413b-f0e8-013f2bb7ec2e"
      },
      "source": [
        "for i in range(len(previsores)):\n",
        "  print(previsores[i], preco_real_teste[i])\n",
        "print(\"Diferença Media em centavos: \" , ((previsores.mean() - preco_real_teste.mean())**2)**1/2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16.173498] [16.190001]\n",
            "[16.251049] [16.49]\n",
            "[16.378012] [16.780001]\n",
            "[16.558084] [16.700001]\n",
            "[16.743559] [16.74]\n",
            "[16.90356] [17.030001]\n",
            "[17.029766] [16.92]\n",
            "[17.089094] [16.879999]\n",
            "[17.116302] [17.040001]\n",
            "[17.154934] [17.32]\n",
            "[17.219614] [17.35]\n",
            "[17.342539] [17.92]\n",
            "[17.589972] [18.35]\n",
            "[17.920555] [18.309999]\n",
            "[18.2189] [18.26]\n",
            "[18.41269] [18.4]\n",
            "[18.467964] [18.42]\n",
            "[18.558918] [19.34]\n",
            "[18.735928] [19.620001]\n",
            "[19.047068] [19.67]\n",
            "[19.407402] [19.77]\n",
            "[19.644646] [19.74]\n",
            "Diferença Media em centavos:  0.02875591711888865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhtaHgtmodN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "de98cd99-6bcc-44fe-95fe-9892931d0af6"
      },
      "source": [
        "\n",
        "plt.plot(preco_real_teste, color = 'red', label = 'Preço real')\n",
        "plt.plot(previsores, color = 'blue', label = 'Previsões')\n",
        "plt.title('Previsão Preco das Ações')\n",
        "plt.xlabel('Tempo')\n",
        "plt.ylabel('Valor Yahoo')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4zxywKEMcYYv/4f8bUf7oDpSYYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}